{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40fac349",
   "metadata": {},
   "source": [
    "# Theoretical of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3ef56d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn Logistic Regression, the coefficients represent the change in the log-odds of the outcome for a one-unit increase in the predictor variable, holding all other variables constant. \\nA positive coefficient indicates that as the predictor increases, the probability of the outcome occurring increases, while a negative coefficient indicates a decrease in probability.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression and Related Concepts\n",
    "\n",
    "# 1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
    "\"\"\"\n",
    "Logistic Regression is a statistical method used for binary classification problems, where the outcome variable is categorical (typically 0 or 1). \n",
    "It predicts the probability that a given input point belongs to a certain class.\n",
    "\n",
    "Difference from Linear Regression:\n",
    "- Output: Linear regression predicts continuous values, while logistic regression predicts probabilities (between 0 and 1).\n",
    "- Function: Linear regression uses a linear function, whereas logistic regression uses the logistic (sigmoid) function to map predicted values to probabilities.\n",
    "\"\"\"\n",
    "\n",
    "# 2. What is the mathematical equation of Logistic Regression?\n",
    "\"\"\"\n",
    "The mathematical equation for logistic regression is:\n",
    "\n",
    "P(Y=1|X) = 1 / (1 + e^-(β0 + β1X1 + β2X2 + ... + βnXn))\n",
    "\n",
    "Where:\n",
    "- P(Y=1|X) is the probability that the output Y is 1 given input features X.\n",
    "- β0 is the intercept.\n",
    "- β1, β2, ..., βn are the coefficients for the features X1, X2, ..., Xn.\n",
    "\"\"\"\n",
    "\n",
    "# 3. Why do we use the Sigmoid function in Logistic Regression?\n",
    "\"\"\"\n",
    "The sigmoid function is used in logistic regression because it maps any real-valued number into the range (0, 1). \n",
    "This is essential for interpreting the output as a probability, which can then be thresholded to make binary classification decisions.\n",
    "\"\"\"\n",
    "\n",
    "# 4. What is the cost function of Logistic Regression?\n",
    "\"\"\"\n",
    "The cost function for logistic regression is the logistic loss (also known as binary cross-entropy loss):\n",
    "\n",
    "J(β) = -1/m * Σ [y^(i) log(h_β(x^(i))) + (1 - y^(i)) log(1 - h_β(x^(i)))]\n",
    "\n",
    "Where:\n",
    "- m is the number of training examples.\n",
    "- y^(i) is the actual label (0 or 1).\n",
    "- h_β(x^(i)) is the predicted probability for the i-th example.\n",
    "\"\"\"\n",
    "\n",
    "# 5. What is Regularization in Logistic Regression? Why is it needed?\n",
    "\"\"\"\n",
    "Regularization is a technique used to prevent overfitting by adding a penalty term to the cost function. \n",
    "It helps to constrain the model complexity.\n",
    "\n",
    "- L1 Regularization (Lasso): Adds the absolute value of the coefficients as a penalty.\n",
    "- L2 Regularization (Ridge): Adds the square of the coefficients as a penalty.\n",
    "\n",
    "Regularization is needed to improve the model's generalization to unseen data.\n",
    "\"\"\"\n",
    "\n",
    "# 6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
    "\"\"\"\n",
    "- Lasso Regression (L1): Adds an L1 penalty, which can lead to sparse models (some coefficients can be exactly zero).\n",
    "- Ridge Regression (L2): Adds an L2 penalty, which shrinks coefficients but does not set them to zero.\n",
    "- Elastic Net: Combines both L1 and L2 penalties, allowing for both feature selection and coefficient shrinkage.\n",
    "\"\"\"\n",
    "\n",
    "# 7. When should we use Elastic Net instead of Lasso or Ridge?\n",
    "\"\"\"\n",
    "Elastic Net should be used when:\n",
    "- There are multiple features that are correlated with each other.\n",
    "- You want to benefit from both L1 and L2 regularization.\n",
    "- You have a high-dimensional dataset where Lasso might select too few features.\n",
    "\"\"\"\n",
    "\n",
    "# 8. What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
    "\"\"\"\n",
    "The regularization parameter (λ) controls the strength of the penalty applied to the coefficients:\n",
    "- A larger λ increases the penalty, leading to more regularization and potentially simpler models.\n",
    "- A smaller λ reduces the penalty, allowing the model to fit the training data more closely, which may lead to overfitting.\n",
    "\"\"\"\n",
    "\n",
    "# 9. What are the key assumptions of Logistic Regression?\n",
    "\"\"\"\n",
    "Key assumptions of Logistic Regression include:\n",
    "- The dependent variable is binary.\n",
    "- The observations are independent.\n",
    "- There is a linear relationship between the logit of the outcome and the independent variables.\n",
    "- No multicollinearity among the independent variables.\n",
    "\"\"\"\n",
    "\n",
    "# 10. What are some alternatives to Logistic Regression for classification tasks?\n",
    "\"\"\"\n",
    "Alternatives to Logistic Regression include:\n",
    "- Decision Trees\n",
    "- Random Forests\n",
    "- Support Vector Machines (SVM)\n",
    "- k-Nearest Neighbors (k-NN)\n",
    "- Neural Networks\n",
    "\"\"\"\n",
    "\n",
    "# 11. What are Classification Evaluation Metrics?\n",
    "\"\"\"\n",
    "Common classification evaluation metrics include:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall (Sensitivity)\n",
    "- F1 Score\n",
    "- ROC-AUC (Receiver Operating Characteristic - Area Under Curve)\n",
    "\"\"\"\n",
    "\n",
    "# 12. How does class imbalance affect Logistic Regression?\n",
    "\"\"\"\n",
    "Class imbalance can lead to biased predictions, where the model may favor the majority class. \n",
    "This can result in high accuracy but poor performance on the minority class. \n",
    "To mitigate this, techniques such as resampling (oversampling the minority class or undersampling the majority class), \n",
    "using different evaluation metrics (like F1 score), or applying class weights can be employed.\n",
    "\"\"\"\n",
    "\n",
    "# 13. What is Hyperparameter Tuning in Logistic Regression?\n",
    "\"\"\"\n",
    "Hyperparameter tuning involves optimizing the parameters that govern the learning process of the model, \n",
    "such as the regularization strength (λ) and the choice of solver. \n",
    "Techniques like Grid Search, Random Search, or Bayesian Optimization can be used to find the best combination of hyperparameters \n",
    "to improve model performance.\n",
    "\"\"\"\n",
    "\n",
    "# 14. What are different solvers in Logistic Regression? Which one should be used?\n",
    "\"\"\"\n",
    "Common solvers for Logistic Regression include:\n",
    "- 'liblinear': Good for small datasets and supports L1 regularization.\n",
    "- 'newton-cg': Suitable for large datasets and supports L2 regularization.\n",
    "- 'lbfgs': An optimization algorithm that is efficient for large datasets and supports L2 regularization.\n",
    "- 'sag': Stochastic Average Gradient descent, suitable for large datasets.\n",
    "- 'saga': Similar to 'sag' but supports L1 regularization.\n",
    "\n",
    "The choice of solver depends on the dataset size and the type of regularization required. \n",
    "For small datasets, 'liblinear' is often a good choice, while 'lbfgs' is preferred for larger datasets.\n",
    "\"\"\"\n",
    "\n",
    "# 15. How is Logistic Regression extended for multiclass classification?\n",
    "\"\"\"\n",
    "Logistic Regression can be extended for multiclass classification using two main approaches:\n",
    "- One-vs-Rest (OvR): A separate binary classifier is trained for each class, treating it as the positive class and all others as negative.\n",
    "- Softmax Regression: A generalization of logistic regression that uses the softmax function to predict probabilities for multiple classes simultaneously.\n",
    "\"\"\"\n",
    "\n",
    "# 16. What are the advantages and disadvantages of Logistic Regression?\n",
    "\"\"\"\n",
    "Advantages:\n",
    "- Simple and easy to implement.\n",
    "- Interpretable coefficients.\n",
    "- Works well with linearly separable data.\n",
    "- Efficient for binary and multiclass classification.\n",
    "\n",
    "Disadvantages:\n",
    "- Assumes a linear relationship between the independent variables and the log-odds of the dependent variable.\n",
    "- Sensitive to outliers.\n",
    "- May not perform well with complex relationships or non-linear data.\n",
    "\"\"\"\n",
    "\n",
    "# 17. What are some use cases of Logistic Regression?\n",
    "\"\"\"\n",
    "Use cases of Logistic Regression include:\n",
    "- Medical diagnosis (e.g., predicting the presence of a disease).\n",
    "- Credit scoring (e.g., predicting loan default).\n",
    "- Marketing (e.g., predicting customer churn).\n",
    "- Spam detection (e.g., classifying emails as spam or not).\n",
    "\"\"\"\n",
    "\n",
    "# 18. What is the difference between Softmax Regression and Logistic Regression?\n",
    "\"\"\"\n",
    "Softmax Regression is a generalization of Logistic Regression for multiclass classification. \n",
    "While Logistic Regression is used for binary classification, Softmax Regression can handle multiple classes by outputting a probability distribution across all classes using the softmax function.\n",
    "\"\"\"\n",
    "\n",
    "# 19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
    "\"\"\"\n",
    "Choosing between OvR and Softmax depends on:\n",
    "- The number of classes: For a large number of classes, Softmax may be more efficient.\n",
    "- The nature of the problem: If classes are not mutually exclusive, OvR may be more appropriate.\n",
    "- Computational resources: Softmax can be computationally intensive for very large datasets.\n",
    "\"\"\"\n",
    "\n",
    "# 20. How do we interpret coefficients in Logistic Regression?\n",
    "\"\"\"\n",
    "In Logistic Regression, the coefficients represent the change in the log-odds of the outcome for a one-unit increase in the predictor variable, holding all other variables constant. \n",
    "A positive coefficient indicates that as the predictor increases, the probability of the outcome occurring increases, while a negative coefficient indicates a decrease in probability.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6783a489",
   "metadata": {},
   "source": [
    "# Practical of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe74666f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load a dataset, split it into training and testing sets, apply Logistic Regression, and print the model accuracy\n",
    "# Using the Iris dataset for demonstration\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Logistic Regression\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c195d754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Regularization Model Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# 2. Apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy\n",
    "model_l1 = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
    "model_l1.fit(X_train, y_train)\n",
    "y_pred_l1 = model_l1.predict(X_test)\n",
    "\n",
    "# Print model accuracy\n",
    "accuracy_l1 = accuracy_score(y_test, y_pred_l1)\n",
    "print(f\"L1 Regularization Model Accuracy: {accuracy_l1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81ea3bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Regularization Model Accuracy: 1.00\n",
      "L2 Coefficients: [[-0.39705946  0.96066053 -2.37396368 -1.00330112]\n",
      " [ 0.51283652 -0.25352109 -0.21527075 -0.76918663]\n",
      " [-0.11577705 -0.70713943  2.58923443  1.77248775]]\n"
     ]
    }
   ],
   "source": [
    "# 3. Train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients\n",
    "model_l2 = LogisticRegression(penalty='l2', max_iter=200)\n",
    "model_l2.fit(X_train, y_train)\n",
    "y_pred_l2 = model_l2.predict(X_test)\n",
    "\n",
    "# Print model accuracy and coefficients\n",
    "accuracy_l2 = accuracy_score(y_test, y_pred_l2)\n",
    "print(f\"L2 Regularization Model Accuracy: {accuracy_l2:.2f}\")\n",
    "print(f\"L2 Coefficients: {model_l2.coef_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5463296b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net Regularization Model Accuracy: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Smart\\anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 4. Train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n",
    "# Note: Elastic Net requires a specific solver. Using 'saga' for this example.\n",
    "model_en = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=200)\n",
    "model_en.fit(X_train, y_train)\n",
    "y_pred_en = model_en.predict(X_test)\n",
    "\n",
    "# Print model accuracy\n",
    "accuracy_en = accuracy_score(y_test, y_pred_en)\n",
    "print(f\"Elastic Net Regularization Model Accuracy: {accuracy_en:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "848f423e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiclass Logistic Regression Model Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# 5. Train a Logistic Regression model for multiclass classification using multi_class='ovr'\n",
    "model_multiclass = LogisticRegression(multi_class='ovr', max_iter=200)\n",
    "model_multiclass.fit(X_train, y_train)\n",
    "y_pred_multiclass = model_multiclass.predict(X_test)\n",
    "\n",
    "# Print model accuracy\n",
    "accuracy_multiclass = accuracy_score(y_test, y_pred_multiclass)\n",
    "print(f\"Multiclass Logistic Regression Model Accuracy: {accuracy_multiclass:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32955b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 10, 'penalty': 'l1'}\n",
      "Best Cross-Validation Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "# 6. Apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "grid_search = GridSearchCV(LogisticRegression(solver='liblinear', max_iter=200), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and accuracy\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "842a4713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy from Stratified K-Fold: 0.97\n"
     ]
    }
   ],
   "source": [
    "# 7. Evaluate Logistic Regression using Stratified K-Fold Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "accuracies = []\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train_cv, X_test_cv = X[train_index], X[test_index]\n",
    "    y_train_cv, y_test_cv = y[train_index], y[test_index]\n",
    "    \n",
    "    model_cv = LogisticRegression(max_iter=200)\n",
    "    model_cv.fit(X_train_cv, y_train_cv)\n",
    "    y_pred_cv = model_cv.predict(X_test_cv)\n",
    "    accuracies.append(accuracy_score(y_test_cv, y_pred_cv))\n",
    "\n",
    "# Print the average accuracy\n",
    "print(f\"Average Accuracy from Stratified K-Fold: {np.mean(accuracies):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10e78213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Smart\\anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Smart\\anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Smart\\anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Smart\\anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'solver': 'saga', 'penalty': 'l1', 'C': 0.1}\n",
      "Best Cross-Validation Accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score, roc_auc_score, cohen_kappa_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# 9. Apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression\n",
    "# Create a synthetic dataset for demonstration\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_dist = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "# Apply RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(LogisticRegression(max_iter=200), param_distributions=param_dist, n_iter=10, cv=5, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and accuracy\n",
    "print(f\"Best Parameters: {random_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {random_search.best_score_:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2eed7fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-vs-One Model Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "# 10. Implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "model_ovo = OneVsOneClassifier(LogisticRegression(max_iter=200))\n",
    "model_ovo.fit(X_train, y_train)\n",
    "y_pred_ovo = model_ovo.predict(X_test)\n",
    "\n",
    "# Print accuracy\n",
    "accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
    "print(f\"One-vs-One Model Accuracy: {accuracy_ovo:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b76c65ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhIAAAHFCAYAAACn7hC1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA530lEQVR4nO3de1xVVf7/8feB9AgKlBcOYpqoaN7yWgxMBmowoePla3et0UzTtCnS0iFL7QbKOGqJaZoXupj5zXSsKZO0MEctNO2CjpXipYkTaSaKhAT794c/z7cTqJzT2R7cvZ4+9uMha6+z1mfTg/j4WWvvbTMMwxAAAIAXAvwdAAAAuHiRSAAAAK+RSAAAAK+RSAAAAK+RSAAAAK+RSAAAAK+RSAAAAK+RSAAAAK+RSAAAAK+RSMDSPvvsM911112KiopSnTp1VK9ePXXt2lUZGRn64YcfTJ17x44dio+PV1hYmGw2m2bPnu3zOWw2m6ZOnerzcc9n6dKlstlsstls+uCDDyqdNwxDrVq1ks1mU0JCgldzPPfcc1q6dKlHn/nggw/OGhMAc1zi7wAAsyxcuFBjxoxRmzZt9PDDD6tdu3YqKyvTtm3bNH/+fG3ZskWrVq0ybf7hw4eruLhYy5cv12WXXabmzZv7fI4tW7bo8ssv9/m41RUSEqJFixZVShZycnK0d+9ehYSEeD32c889p4YNG2rYsGHV/kzXrl21ZcsWtWvXzut5AXiGRAKWtGXLFt17771KTEzU6tWrZbfbXecSExM1fvx4rV271tQYvvjiC40cOVLJycmmzfGHP/zBtLGr49Zbb9Urr7yiuXPnKjQ01NW+aNEixcbGqqio6ILEUVZWJpvNptDQUL9/T4DfG5Y2YElpaWmy2WxasGCBWxJxRu3atdW/f3/X1xUVFcrIyNCVV14pu92u8PBw/eUvf9E333zj9rmEhAR16NBBubm56tGjh4KDg9WiRQtNmzZNFRUVkv6v7P/zzz9r3rx5riUASZo6darr77905jP79+93tW3YsEEJCQlq0KCBgoKC1KxZM9144406efKkq09VSxtffPGFBgwYoMsuu0x16tRR586dlZWV5dbnzBLAq6++qkmTJikyMlKhoaG6/vrrtWfPnup9kyXdfvvtkqRXX33V1Xbs2DGtXLlSw4cPr/Izjz/+uGJiYlS/fn2Fhoaqa9euWrRokX75/sDmzZsrLy9POTk5ru/fmYrOmdhfeukljR8/Xk2aNJHdbtfXX39daWnj8OHDatq0qeLi4lRWVuYaf9euXapbt67uvPPOal8rgKqRSMByysvLtWHDBnXr1k1Nmzat1mfuvfdeTZw4UYmJiVqzZo2efPJJrV27VnFxcTp8+LBbX6fTqSFDhuiOO+7QmjVrlJycrNTUVL388suSpL59+2rLli2SpJtuuklbtmxxfV1d+/fvV9++fVW7dm0tXrxYa9eu1bRp01S3bl2dOnXqrJ/bs2eP4uLilJeXp2effVZvvPGG2rVrp2HDhikjI6NS/0ceeUQHDhzQCy+8oAULFuirr75Sv379VF5eXq04Q0NDddNNN2nx4sWutldffVUBAQG69dZbz3pto0aN0ooVK/TGG29o0KBB+utf/6onn3zS1WfVqlVq0aKFunTp4vr+/XoZKjU1VQcPHtT8+fP15ptvKjw8vNJcDRs21PLly5Wbm6uJEydKkk6ePKmbb75ZzZo10/z586t1nQDOwQAsxul0GpKM2267rVr9d+/ebUgyxowZ49b+0UcfGZKMRx55xNUWHx9vSDI++ugjt77t2rUz/vSnP7m1STLGjh3r1jZlyhSjqh+7JUuWGJKM/Px8wzAM4/XXXzckGTt37jxn7JKMKVOmuL6+7bbbDLvdbhw8eNCtX3JyshEcHGz8+OOPhmEYxvvvv29IMvr06ePWb8WKFYYkY8uWLeec90y8ubm5rrG++OILwzAM4+qrrzaGDRtmGIZhtG/f3oiPjz/rOOXl5UZZWZnxxBNPGA0aNDAqKipc58722TPzXXfddWc99/7777u1T58+3ZBkrFq1yhg6dKgRFBRkfPbZZ+e8RgDVQ0UCv3vvv/++JFXa1HfNNdeobdu2Wr9+vVt7RESErrnmGre2q666SgcOHPBZTJ07d1bt2rV1zz33KCsrS/v27avW5zZs2KDevXtXqsQMGzZMJ0+erFQZ+eXyjnT6OiR5dC3x8fFq2bKlFi9erM8//1y5ublnXdY4E+P111+vsLAwBQYGqlatWpo8ebKOHDmiwsLCas974403Vrvvww8/rL59++r2229XVlaW5syZo44dO1b78wDOjkQCltOwYUMFBwcrPz+/Wv2PHDkiSWrcuHGlc5GRka7zZzRo0KBSP7vdrpKSEi+irVrLli313nvvKTw8XGPHjlXLli3VsmVLPfPMM+f83JEjR856HWfO/9Kvr+XMfhJPrsVms+muu+7Syy+/rPnz56t169bq0aNHlX0//vhjJSUlSTp9V82///1v5ebmatKkSR7PW9V1nivGYcOG6aefflJERAR7IwAfIpGA5QQGBqp3797avn17pc2SVTnzy7SgoKDSuW+//VYNGzb0WWx16tSRJJWWlrq1/3ofhiT16NFDb775po4dO6atW7cqNjZWKSkpWr58+VnHb9CgwVmvQ5JPr+WXhg0bpsOHD2v+/Pm66667ztpv+fLlqlWrlt566y3dcsstiouLU/fu3b2as6pNq2dTUFCgsWPHqnPnzjpy5Igeeughr+YEUBmJBCwpNTVVhmFo5MiRVW5OLCsr05tvvilJ6tWrlyS5NkuekZubq927d6t3794+i+vMnQefffaZW/uZWKoSGBiomJgYzZ07V5L0ySefnLVv7969tWHDBlficMaLL76o4OBg026NbNKkiR5++GH169dPQ4cOPWs/m82mSy65RIGBga62kpISvfTSS5X6+qrKU15erttvv102m03vvPOO0tPTNWfOHL3xxhu/eWwAPEcCFhUbG6t58+ZpzJgx6tatm+699161b99eZWVl2rFjhxYsWKAOHTqoX79+atOmje655x7NmTNHAQEBSk5O1v79+/XYY4+padOmevDBB30WV58+fVS/fn3dfffdeuKJJ3TJJZdo6dKlOnTokFu/+fPna8OGDerbt6+aNWumn376yXVnxPXXX3/W8adMmaK33npLPXv21OTJk1W/fn298sor+te//qWMjAyFhYX57Fp+bdq0aeft07dvX82cOVODBw/WPffcoyNHjmjGjBlV3qLbsWNHLV++XK+99ppatGihOnXqeLWvYcqUKfrwww+1bt06RUREaPz48crJydHdd9+tLl26KCoqyuMxAfwfEglY1siRI3XNNddo1qxZmj59upxOp2rVqqXWrVtr8ODBuu+++1x9582bp5YtW2rRokWaO3euwsLCdMMNNyg9Pb3KPRHeCg0N1dq1a5WSkqI77rhDl156qUaMGKHk5GSNGDHC1a9z585at26dpkyZIqfTqXr16qlDhw5as2aNa49BVdq0aaPNmzfrkUce0dixY1VSUqK2bdtqyZIlHj0h0iy9evXS4sWLNX36dPXr109NmjTRyJEjFR4errvvvtut7+OPP66CggKNHDlSx48f1xVXXOH2nI3qyM7OVnp6uh577DG3ytLSpUvVpUsX3Xrrrdq0aZNq167ti8sDfpdshvGLp8AAAAB4gD0SAADAayQSAADAayQSAADAayQSAADAayQSAADAayQSAADAayQSAADAa5Z8IFVQd989iRCwkqNbZ/k7BKDGqXMBfhMGdbnv/J2qoWRHpk/G8SUqEgAAwGuWrEgAAFCj2Kz773YSCQAAzObBa+8vNiQSAACYzcIVCeteGQAAMB0VCQAAzMbSBgAA8BpLGwAAAJVRkQAAwGwsbQAAAK+xtAEAAFAZFQkAAMzG0gYAAPAaSxsAAACVUZEAAMBsLG0AAACvWXhpg0QCAACzWbgiYd0UCQAAmI6KBAAAZmNpAwAAeM3CiYR1rwwAAJiOigQAAGYLsO5mSxIJAADMxtIGAAC4mPz888969NFHFRUVpaCgILVo0UJPPPGEKioqXH0Mw9DUqVMVGRmpoKAgJSQkKC8vz6N5SCQAADCbzeabwwPTp0/X/PnzlZmZqd27dysjI0N///vfNWfOHFefjIwMzZw5U5mZmcrNzVVERIQSExN1/Pjxas/D0gYAAGbzw9LGli1bNGDAAPXt21eS1Lx5c7366qvatm2bpNPViNmzZ2vSpEkaNGiQJCkrK0sOh0PLli3TqFGjqjUPFQkAACzo2muv1fr16/Xll19Kkj799FNt2rRJffr0kSTl5+fL6XQqKSnJ9Rm73a74+Hht3ry52vNQkQAAwGw+ekR2aWmpSktL3drsdrvsdnulvhMnTtSxY8d05ZVXKjAwUOXl5Xr66ad1++23S5KcTqckyeFwuH3O4XDowIED1Y6JigQAAGazBfjkSE9PV1hYmNuRnp5e5ZSvvfaaXn75ZS1btkyffPKJsrKyNGPGDGVlZbmH9qskxzCMSm3nQkUCAACz+agikZqaqnHjxrm1VVWNkKSHH35Yf/vb33TbbbdJkjp27KgDBw4oPT1dQ4cOVUREhKTTlYnGjRu7PldYWFipSnEuVCQAALhI2O12hYaGuh1nSyROnjypgAD3X/OBgYGu2z+joqIUERGh7Oxs1/lTp04pJydHcXFx1Y6JigQAAGbzw10b/fr109NPP61mzZqpffv22rFjh2bOnKnhw4efDslmU0pKitLS0hQdHa3o6GilpaUpODhYgwcPrvY8JBIAAJjNR0sbnpgzZ44ee+wxjRkzRoWFhYqMjNSoUaM0efJkV58JEyaopKREY8aM0dGjRxUTE6N169YpJCSk2vPYDMMwzLgAfwrq/qC/QwBqpKNbZ/k7BKDGqXMB/kkdlOybn72Sd2re7zcqEgAAmM3C79ogkQAAwGx+WNq4UKybIgEAANNRkQAAwGwsbQAAAK9ZOJGw7pUBAADTUZEAAMBsFt5sSSIBAIDZLLy0QSIBAIDZLFyRsG6KBAAATEdFAgAAs7G0AQAAvMbSBgAAQGVUJAAAMJnNwhUJEgkAAExm5USCpQ0AAOA1KhIAAJjNugUJEgkAAMzG0gYAAEAVqEgAAGAyK1ckSCQAADAZiQQAAPCalRMJ9kgAAACvUZEAAMBs1i1IkEgAAGA2ljYAAACqQEUCAACTWbkiQSIBAIDJrJxIsLQBAAC8RkUCAACTWbkiQSIBAIDZrJtHsLQBAAC8R0UCAACTsbQBAAC8RiIBAAC8ZuVEgj0SAADAa1QkAAAwm3ULElQkAAAwm81m88nhiebNm1c5xtixYyVJhmFo6tSpioyMVFBQkBISEpSXl+fxtZFIAABgQbm5uSooKHAd2dnZkqSbb75ZkpSRkaGZM2cqMzNTubm5ioiIUGJioo4fP+7RPCQSAACYzB8ViUaNGikiIsJ1vPXWW2rZsqXi4+NlGIZmz56tSZMmadCgQerQoYOysrJ08uRJLVu2zKN5SCQAADCZrxKJ0tJSFRUVuR2lpaXnnf/UqVN6+eWXNXz4cNlsNuXn58vpdCopKcnVx263Kz4+Xps3b/bo2kgkAAC4SKSnpyssLMztSE9PP+/nVq9erR9//FHDhg2TJDmdTkmSw+Fw6+dwOFznqou7NgAAMJmvniORmpqqcePGubXZ7fbzfm7RokVKTk5WZGTkOeMyDMPjWEkkAAAwm49u/7Tb7dVKHH7pwIEDeu+99/TGG2+42iIiIiSdrkw0btzY1V5YWFipSnE+LG0AAGBhS5YsUXh4uPr27etqi4qKUkREhOtODun0PoqcnBzFxcV5ND4VCQAATOavR2RXVFRoyZIlGjp0qC655P9+5dtsNqWkpCgtLU3R0dGKjo5WWlqagoODNXjwYI/mIJEAAMBk/kok3nvvPR08eFDDhw+vdG7ChAkqKSnRmDFjdPToUcXExGjdunUKCQnxaA6bYRiGrwKuKYK6P+jvEIAa6ejWWf4OAahx6lyAf1I3HftPn4xzaO4An4zjS+yRAAAAXmNpAwAAs1n4pV0kEgAAmMxfeyQuBJY2AACA16hI4DcLDAzQo/f8Sbfd0E2OBiFyHj6ul976WNMWZevMXt4FU27Xnf2ucfvcx5/vV/xdz/gjZOCC+/nnnzV/7hz9619v6sjhw2rYqJH6D/gf3TN6jAIC+Ded1Vm5IkEigd9s/NBeGnFjnEZOeVW79hWoW7tmen7ybSo68ZPmLt/o6vfuv3dr1BOvur4+VVbuj3ABv1iyaKH+d8VyPZk2XS1btdKuL77Q5EdTFRISoiF3DvV3eDAZiQRwDjEdm+utnC+09t+7JEkHC47qlj91Udd2Td36nSr7Wd8d8ew994BVfPrpTiX06q3r4hMkSU2aXK533v6X8vK+8G9gwG/k13raN998o0mTJqlnz55q27at2rVrp549e2rSpEk6dOiQP0ODB7bszFfPq1urVbNGkqSO0ZGK7dRC7/7/xOKMHt1a6cC6J/TZylTNnXSLGl1Wzx/hAn7RpUs3fbx1q/bvz5ck7fnPf7Rjx3b16BHv58hwIfjqNeI1kd8qEps2bVJycrKaNm2qpKQkJSUlyTAMFRYWavXq1ZozZ47eeecd/fGPf/RXiKimGVnrFVqvjj59/W8qrzAUGGDTlOfe1op3d7j6rNu8W2+896kOOn9Q88gGmjw6We/MH6O4O/7BEgd+F4aPGKkTJ45r4J+TFRgYqPLycv31gQeV3PfP/g4NF0LNzAF8wm+JxIMPPqgRI0Zo1qyqn7T34IMPKiUlRbm5ueccp7S0VKWlpW5tRsXPsgWwanOh3JzURbcnd9OwR1/Wrr1OXdWmif4+bqAKvi/SK/86/d/v9eydrv679jr1ya5D2vPWY0q+tp3++f7nfoocuHDWvvO2/vXWGqVn/EOtWrXSf/6zW3+flq5GjcLVf+D/+Ds8wGt++237xRdf6OWXXz7r+VGjRmn+/PnnHSc9PV2PP/64W1tg4xjVioz9zTGietLu76cZWev1v+tOVyDy9haoWePL9PBdvV2JxK85jxTpYMFR13IIYHWz/pGh4Xffo+Q+p9/AGN26jQq+/VaLXnieROJ3oKYuS/iC3/ZING7cWJs3bz7r+S1btri9I/1sUlNTdezYMbfjkoirfRkqziOoTm1VVLi/sqW8vEIB5/jBqR8WrMsdl6rgcJHZ4QE1wk8lPykgwP1nIjAwsNLPDqyJPRImeOihhzR69Ght375diYmJcjgcstlscjqdys7O1gsvvKDZs2efdxy73S673e7WxrLGhfX2h3maODxRh5w/ate+AnVuc7nuH5KgF9d8JEmqG1Rbj95zg1Zv+FQFh4t0RWR9PTGmr478WKw1LGvgdyI+oacWLpiviMaRatmqlf6ze7deylqiAf9zo79DwwVQQ3MAn/Dr2z9fe+01zZo1S9u3b1d5+ekNd4GBgerWrZvGjRunW265xatxefvnhVUv2K4po5PVv2dHNbqsngoOF2nFu58obeE6lf1crjr2WloxY7g6tWmiS0OC5DxcpJxtX+uJ+e/om+9+9Hf4vyu8/dN/iotPaO6zz2jD+vf0ww9H1Cg8XMnJfTXq3rGqVbu2v8P7XbsQb/9s9dA7Phnn6xnJPhnHl2rEa8TLysp0+PBhSVLDhg1Vq1at3zQeiQRQNRIJoLILkUhEP7zWJ+N89fcbfDKOL9WINYBatWpVaz8EAAAXIysvbfCAdwAA4LUaUZEAAMDKauodF75AIgEAgMksnEewtAEAALxHRQIAAJP9+mFkVkIiAQCAyVjaAAAAqAIVCQAATMZdGwAAwGsWziNIJAAAMJuVKxLskQAAAF6jIgEAgMmsXJEgkQAAwGQWziNY2gAAAN6jIgEAgMlY2gAAAF6zcB7B0gYAAPAeFQkAAEzG0gYAAPCahfMIljYAAID3SCQAADCZzWbzyeGp//73v7rjjjvUoEEDBQcHq3Pnztq+fbvrvGEYmjp1qiIjIxUUFKSEhATl5eV5NAeJBAAAJrPZfHN44ujRo/rjH/+oWrVq6Z133tGuXbv0j3/8Q5deeqmrT0ZGhmbOnKnMzEzl5uYqIiJCiYmJOn78eLXnYY8EAAAm88dmy+nTp6tp06ZasmSJq6158+auvxuGodmzZ2vSpEkaNGiQJCkrK0sOh0PLli3TqFGjqjUPFQkAAC4SpaWlKioqcjtKS0ur7LtmzRp1795dN998s8LDw9WlSxctXLjQdT4/P19Op1NJSUmuNrvdrvj4eG3evLnaMZFIAABgMl8tbaSnpyssLMztSE9Pr3LOffv2ad68eYqOjta7776r0aNH6/7779eLL74oSXI6nZIkh8Ph9jmHw+E6Vx0sbQAAYDJfLW2kpqZq3Lhxbm12u73KvhUVFerevbvS0tIkSV26dFFeXp7mzZunv/zlL2eNzTAMj+KlIgEAwEXCbrcrNDTU7ThbItG4cWO1a9fOra1t27Y6ePCgJCkiIkKSKlUfCgsLK1UpzoVEAgAAk/njro0//vGP2rNnj1vbl19+qSuuuEKSFBUVpYiICGVnZ7vOnzp1Sjk5OYqLi6v2PCxtAABgMn/ctfHggw8qLi5OaWlpuuWWW/Txxx9rwYIFWrBggSumlJQUpaWlKTo6WtHR0UpLS1NwcLAGDx5c7XlIJAAAsKCrr75aq1atUmpqqp544glFRUVp9uzZGjJkiKvPhAkTVFJSojFjxujo0aOKiYnRunXrFBISUu15bIZhGGZcgD8FdX/Q3yEANdLRrbP8HQJQ49S5AP+kvnbGhz4ZZ9NDPXwyji9RkQAAwGRWfvsnmy0BAIDXqEgAAGAyK1ckSCQAADCZhfMIEgkAAMxm5YoEeyQAAIDXqEgAAGAyCxckSCQAADAbSxsAAABVoCIBAIDJLFyQIJEAAMBsARbOJFjaAAAAXqMiAQCAySxckCCRAADAbFa+a4NEAgAAkwVYN49gjwQAAPAeFQkAAEzG0gYAAPCahfMIljYAAID3qEgAAGAym6xbkiCRAADAZNy1AQAAUAUqEgAAmIy7NgAAgNcsnEewtAEAALxHRQIAAJNZ+TXiJBIAAJjMwnkEiQQAAGaz8mZL9kgAAACvUZEAAMBkFi5IkEgAAGA2K2+2ZGkDAAB4jYoEAAAms249gkQCAADTcdcGAABAFahIAABgMiu/RrxaicSaNWuqPWD//v29DgYAACvyx9LG1KlT9fjjj7u1ORwOOZ1OSZJhGHr88ce1YMECHT16VDExMZo7d67at2/v0TzVSiQGDhxYrcFsNpvKy8s9CgAAAJijffv2eu+991xfBwYGuv6ekZGhmTNnaunSpWrdurWeeuopJSYmas+ePQoJCan2HNVKJCoqKjwIGwAA/JK/9lpecsklioiIqNRuGIZmz56tSZMmadCgQZKkrKwsORwOLVu2TKNGjar2HGy2BADAZDabzSeHp7766itFRkYqKipKt912m/bt2ydJys/Pl9PpVFJSkquv3W5XfHy8Nm/e7NEcXm22LC4uVk5Ojg4ePKhTp065nbv//vu9GRIAAMvy1WbL0tJSlZaWurXZ7XbZ7fZKfWNiYvTiiy+qdevW+u677/TUU08pLi5OeXl5rn0SDofD7TMOh0MHDhzwKCaPE4kdO3aoT58+OnnypIqLi1W/fn0dPnxYwcHBCg8PJ5EAAMAk6enplTZQTpkyRVOnTq3UNzk52fX3jh07KjY2Vi1btlRWVpb+8Ic/SKq8CdQwDI8rHx4vbTz44IPq16+ffvjhBwUFBWnr1q06cOCAunXrphkzZng6HAAAluerpY3U1FQdO3bM7UhNTa1WDHXr1lXHjh311VdfufZNnKlMnFFYWFipSnE+HicSO3fu1Pjx4xUYGKjAwECVlpaqadOmysjI0COPPOLpcAAAWJ7NR4fdbldoaKjbUdWyRlVKS0u1e/duNW7cWFFRUYqIiFB2drbr/KlTp5STk6O4uDiPrs3jRKJWrVqusofD4dDBgwclSWFhYa6/AwAA/3rooYeUk5Oj/Px8ffTRR7rppptUVFSkoUOHymazKSUlRWlpaVq1apW++OILDRs2TMHBwRo8eLBH83i8R6JLly7atm2bWrdurZ49e2ry5Mk6fPiwXnrpJXXs2NHT4QAAsDx/vEb8m2++0e23367Dhw+rUaNG+sMf/qCtW7fqiiuukCRNmDBBJSUlGjNmjOuBVOvWrfPoGRKSZDMMw/DkA9u2bdPx48fVs2dPff/99xo6dKg2bdqkVq1aacmSJerUqZNHAZghqPuD/g4BqJGObp3l7xCAGqfOBXhZxMgVX/hknIW3dPDJOL7k8beve/furr83atRIb7/9tk8DAgAAFw9e2gUAgMms/BpxjxOJqKioc35Dzjw1CwAAnGbhPMLzRCIlJcXt67KyMu3YsUNr167Vww8/7Ku4AADARcDjROKBBx6osn3u3Lnatm3bbw4IAACr8cddGxeKz17alZycrJUrV/pqOAAALMNm881RE/lss+Xrr7+u+vXr+2o4AAAsg82Wv9ClSxe3b4hhGHI6nfr+++/13HPP+TQ4AABQs3mcSAwYMMAtkQgICFCjRo2UkJCgK6+80qfBeeuTN5/ydwhAjXTZ1ff5OwSgxinZkWn6HD7bR1ADeZxIVPWqUgAAcHZWXtrwOEkKDAxUYWFhpfYjR44oMDDQJ0EBAICLg8cVibO9mqO0tFS1a9f+zQEBAGA1AdYtSFQ/kXj22WclnS7PvPDCC6pXr57rXHl5uTZu3Fhj9kgAAFCTkEhImjXr9FsDDcPQ/Pnz3ZYxateurebNm2v+/Pm+jxAAANRY1U4k8vPzJUk9e/bUG2+8ocsuu8y0oAAAsBIrb7b0eI/E+++/b0YcAABYlpWXNjy+a+Omm27StGnTKrX//e9/18033+yToAAAwMXB40QiJydHffv2rdR+ww03aOPGjT4JCgAAK+FdG79w4sSJKm/zrFWrloqKinwSFAAAVsLbP3+hQ4cOeu211yq1L1++XO3atfNJUAAAWEmAj46ayOOKxGOPPaYbb7xRe/fuVa9evSRJ69ev17Jly/T666/7PEAAAFBzeZxI9O/fX6tXr1ZaWppef/11BQUFqVOnTtqwYYNCQ0PNiBEAgIuahVc2PE8kJKlv376uDZc//vijXnnlFaWkpOjTTz9VeXm5TwMEAOBixx6JKmzYsEF33HGHIiMjlZmZqT59+mjbtm2+jA0AANRwHlUkvvnmGy1dulSLFy9WcXGxbrnlFpWVlWnlypVstAQA4CwsXJCofkWiT58+ateunXbt2qU5c+bo22+/1Zw5c8yMDQAASwiw+eaoiapdkVi3bp3uv/9+3XvvvYqOjjYzJgAAcJGodkXiww8/1PHjx9W9e3fFxMQoMzNT33//vZmxAQBgCQE2m0+OmqjaiURsbKwWLlyogoICjRo1SsuXL1eTJk1UUVGh7OxsHT9+3Mw4AQC4aFn5Edke37URHBys4cOHa9OmTfr88881fvx4TZs2TeHh4erfv78ZMQIAgBrqNz1xs02bNsrIyNA333yjV1991VcxAQBgKWy2PI/AwEANHDhQAwcO9MVwAABYik01NAvwAZ8kEgAA4OxqajXBF2rqy8QAAMBFgIoEAAAms3JFgkQCAACT2WrqvZs+wNIGAADwGokEAAAmqwm3f6anp8tmsyklJcXVZhiGpk6dqsjISAUFBSkhIUF5eXmeXdtvCwsAAJyPv59smZubqwULFuiqq65ya8/IyNDMmTOVmZmp3NxcRUREKDEx0aOnVZNIAABgYSdOnNCQIUO0cOFCXXbZZa52wzA0e/ZsTZo0SYMGDVKHDh2UlZWlkydPatmyZdUen0QCAACT+eqlXaWlpSoqKnI7SktLzzn32LFj1bdvX11//fVu7fn5+XI6nUpKSnK12e12xcfHa/PmzdW/Ns++FQAAwFO+2iORnp6usLAwtyM9Pf2s8y5fvlyffPJJlX2cTqckyeFwuLU7HA7Xuerg9k8AAC4SqampGjdunFub3W6vsu+hQ4f0wAMPaN26dapTp85Zx/z1ramGYXh0uyqJBAAAJvPVYyTsdvtZE4df2759uwoLC9WtWzdXW3l5uTZu3KjMzEzt2bNH0unKROPGjV19CgsLK1UpzoWlDQAATBYgm08OT/Tu3Vuff/65du7c6Tq6d++uIUOGaOfOnWrRooUiIiKUnZ3t+sypU6eUk5OjuLi4as9DRQIAAJP548GWISEh6tChg1tb3bp11aBBA1d7SkqK0tLSFB0drejoaKWlpSk4OFiDBw+u9jwkEgAA/E5NmDBBJSUlGjNmjI4ePaqYmBitW7dOISEh1R7DZhiGYWKMfrG7oNjfIQA1Utc+E/0dAlDjlOzINH2O+Vv2+2Sc0bHNfTKOL1GRAADAZAG8tAsAAKAyKhIAAJjMwgUJEgkAAMzG0gYAAEAVqEgAAGAyCxckSCQAADCblcv/Vr42AABgMioSAACYzJO3aV5sSCQAADCZddMIEgkAAEzH7Z8AAABVoCIBAIDJrFuPIJEAAMB0Fl7ZYGkDAAB4j4oEAAAm4/ZPAADgNSuX/618bQAAwGRUJAAAMBlLGwAAwGvWTSNY2gAAAL8BFQkAAEzG0gYAAPCalcv/JBIAAJjMyhUJKydJAADAZFQkAAAwmXXrESQSAACYzsIrGyxtAAAA71GRAADAZAEWXtwgkQAAwGQsbQAAAFSBigQAACazsbQBAAC8xdIGAABAFahIAABgMu7aAAAAXmNpAwAAeM1m883hiXnz5umqq65SaGioQkNDFRsbq3feecd13jAMTZ06VZGRkQoKClJCQoLy8vI8vjYSCQAALOjyyy/XtGnTtG3bNm3btk29evXSgAEDXMlCRkaGZs6cqczMTOXm5ioiIkKJiYk6fvy4R/OQSAAAYDKbj/54ol+/furTp49at26t1q1b6+mnn1a9evW0detWGYah2bNna9KkSRo0aJA6dOigrKwsnTx5UsuWLfNoHhIJAABMFmDzzVFaWqqioiK3o7S09Lzzl5eXa/ny5SouLlZsbKzy8/PldDqVlJTk6mO32xUfH6/Nmzd7dm0efzcAAIBfpKenKywszO1IT08/a//PP/9c9erVk91u1+jRo7Vq1Sq1a9dOTqdTkuRwONz6OxwO17nq4q4NAABM5qsnW6ampmrcuHFubXa7/az927Rpo507d+rHH3/UypUrNXToUOXk5PxfXL/awWkYRqW28yGRAADAZL66/dNut58zcfi12rVrq1WrVpKk7t27Kzc3V88884wmTpwoSXI6nWrcuLGrf2FhYaUqxfmwtAEAwO+EYRgqLS1VVFSUIiIilJ2d7Tp36tQp5eTkKC4uzqMxqUgAAGAyf7y065FHHlFycrKaNm2q48ePa/ny5frggw+0du1a2Ww2paSkKC0tTdHR0YqOjlZaWpqCg4M1ePBgj+YhkQAAwGQBfniy5Xfffac777xTBQUFCgsL01VXXaW1a9cqMTFRkjRhwgSVlJRozJgxOnr0qGJiYrRu3TqFhIR4NI/NMAzDjAvwp90Fxf4OAaiRuvaZ6O8QgBqnZEem6XNs/PIHn4xzXev6PhnHl6hI4Dd7/ZXF2rpxg745uF92u11t2nfS0FH3q0mz5q4+Wzau17tvrtTePf/R8aIfNXPhq2oR3cZ/QQMmCwwM0KOj+ui2Pt3laBAq5+EivfTmVk1b+K5++e+3NlEOPfXAQPXo2koBATbt3lugOyYu1iHnUT9GD1/zx9LGhUIigd8sb+d2JQ+8RdFXtld5ebleeSFTUx8eozlLV6pOUJAk6aefStS2Q2f9MT5Rc2c86eeIAfONH5aoETddq5GTX9KuvQXq1r6Znp96h4qO/6S5r34gSYq6vKHWLx6nrNWb9dS8f+nYiRJdGRWhn0rL/Bs8fM7KL+0ikcBvNuXvc92+/uvfHtfQgb2198tdat+pmySpZ9KfJUnfFXx7weMD/CHmqii9lfOZ1m46/V6DgwU/6JYbuqtru2auPo/f10/vbsrTpGf+6Wrb/98jFzxWmM/CeQS3f8L3Tp44/cKXeiFhfo4E8J8tO/eq5zVt1KpZuCSpY+smiu3cQu/++3RiYbPZdMO17fXVwUKtmTtWB9ana+OLD6lfwlX+DBvwWI1OJA4dOqThw4efs09Vzx0/VY3njsMchmFo8XMz1bZjZ13RopW/wwH8ZsaSbK1Yu12frnpURR8/o62vTlTmsg+0Yu12SVJ4/XoKqVtHD92VqOzNu9Tv3kytef9TLf/HCF3bjZ8dqwmw2Xxy1EQ1OpH44YcflJWVdc4+VT13fMGcGRcoQvzagmemaf/erzT+sbM/+x34Pbj5T910e5+rNeyRLMUOnq4Rk19Syp29NaRfjCQpIOD0/37f+uBzzXnlfX325X81Y0m23v4wTyNvutafocMENh8dNZFf90isWbPmnOf37dt33jGqeu54/g8//6a44J0Fz0zXx//eqLRnX1DDcM8esQpYTVrKQM1Ykq3/ffd0BSLv62/VrHF9PXxXol558yMdPnpCZWXl2r2vwO1ze/Y5FdelhT9CBrzi10Ri4MCBstlsOtejLM738pCqnjteu5jnSFxIhmFo4TPTtXXT+3pq9kI5Gjfxd0iA3wXVqa0Ko8KtrbzCcFUiyn4u1/ZdB9T6CvekO/qKcB0s4NZPy6mp5QQf8OvSRuPGjbVy5UpVVFRUeXzyySf+DA/V9Pzsafog+22NezRNQUHBOnrksI4eOazS0p9cfY4XHdO+r/bo0IHTVaZvD+3Xvq/26OiRw/4KGzDV2xs/18S7/6Qbrm2vZo3rq3/Pq3T/HT21ZsOnrj6zst7TTX/qqrv+J04tmjbU6FuvU5/rOmjBio1+jBxmsPnoT03k1ydb9u/fX507d9YTTzxR5flPP/1UXbp0UUVFRZXnz4YnW15YAxO6Vtn+14lT1Tu5vyRp/TtrNGf61Ep9bh16j26/a7SZ4eEXeLLlhVMv2K4pY/6s/r06qdFl9VTw/TGtWLtdaQveUdnP5a5+fxnwBz08PElNwi/VlwcK9dT8f+mtDz73Y+S/PxfiyZYf7T3mk3FiWta8u+H8mkh8+OGHKi4u1g033FDl+eLiYm3btk3x8fEejUsiAVSNRAKo7EIkEh/v800icU2LmpdI+HWPRI8ePc55vm7duh4nEQAA1DQ1c1HCN2r07Z8AAKBm4xHZAACYzcIlCRIJAABMVlPvuPAFEgkAAExWQ59u7RPskQAAAF6jIgEAgMksXJAgkQAAwHQWziRY2gAAAF6jIgEAgMm4awMAAHiNuzYAAACqQEUCAACTWbggQSIBAIDpLJxJsLQBAAC8RkUCAACTcdcGAADwmpXv2iCRAADAZBbOI9gjAQAAvEdFAgAAs1m4JEEiAQCAyay82ZKlDQAA4DUqEgAAmIy7NgAAgNcsnEewtAEAALxHRQIAALNZuCRBRQIAAJPZfPTHE+np6br66qsVEhKi8PBwDRw4UHv27HHrYxiGpk6dqsjISAUFBSkhIUF5eXkezUMiAQCABeXk5Gjs2LHaunWrsrOz9fPPPyspKUnFxcWuPhkZGZo5c6YyMzOVm5uriIgIJSYm6vjx49Wex2YYhmHGBfjT7oLi83cCfoe69pno7xCAGqdkR6bpc+xxnvTJOG0igr3+7Pfff6/w8HDl5OTouuuuk2EYioyMVEpKiiZOPP3/htLSUjkcDk2fPl2jRo2q1rhUJAAAMJnNR0dpaamKiorcjtLS0mrFcOzYMUlS/fr1JUn5+flyOp1KSkpy9bHb7YqPj9fmzZurfW0kEgAAmM1HmUR6errCwsLcjvT09PNObxiGxo0bp2uvvVYdOnSQJDmdTkmSw+Fw6+twOFznqoO7NgAAuEikpqZq3Lhxbm12u/28n7vvvvv02WefadOmTZXO2X71tCzDMCq1nQuJBAAAJvPVuzbsdnu1Eodf+utf/6o1a9Zo48aNuvzyy13tERERkk5XJho3buxqLywsrFSlOBeWNgAAMJnN5pvDE4Zh6L777tMbb7yhDRs2KCoqyu18VFSUIiIilJ2d7Wo7deqUcnJyFBcXV+15qEgAAGBBY8eO1bJly/TPf/5TISEhrn0PYWFhCgoKks1mU0pKitLS0hQdHa3o6GilpaUpODhYgwcPrvY8JBIAAJjMHw+2nDdvniQpISHBrX3JkiUaNmyYJGnChAkqKSnRmDFjdPToUcXExGjdunUKCQmp9jw8RwL4HeE5EkBlF+I5Enu/L/HJOC0bBflkHF9ijwQAAPAaSxsAAJjMV3dt1EQkEgAAmMzTOy4uJixtAAAAr1GRAADAZBYuSJBIAABgOgtnEiQSAACYzMqbLdkjAQAAvEZFAgAAk1n5rg0SCQAATGbhPIKlDQAA4D0qEgAAmIylDQAA8BtYN5NgaQMAAHiNigQAACZjaQMAAHjNwnkESxsAAMB7VCQAADAZSxsAAMBrVn7XBokEAABms24ewR4JAADgPSoSAACYzMIFCRIJAADMZuXNlixtAAAAr1GRAADAZNy1AQAAvGfdPIKlDQAA4D0qEgAAmMzCBQkSCQAAzMZdGwAAAFWgIgEAgMm4awMAAHiNpQ0AAIAqkEgAAACvsbQBAIDJrLy0QSIBAIDJrLzZkqUNAAAsauPGjerXr58iIyNls9m0evVqt/OGYWjq1KmKjIxUUFCQEhISlJeX59EcJBIAAJjMZvPN4ani4mJ16tRJmZmZVZ7PyMjQzJkzlZmZqdzcXEVERCgxMVHHjx+v9hwsbQAAYDJ/LWwkJycrOTm5ynOGYWj27NmaNGmSBg0aJEnKysqSw+HQsmXLNGrUqGrNQUUCAICLRGlpqYqKityO0tJSr8bKz8+X0+lUUlKSq81utys+Pl6bN2+u9jgkEgAAmM3mmyM9PV1hYWFuR3p6ulchOZ1OSZLD4XBrdzgcrnPVwdIGAAAm89VdG6mpqRo3bpxbm91u/01j2n61+cIwjEpt50IiAQDARcJut//mxOGMiIgISacrE40bN3a1FxYWVqpSnAtLGwAAmMxfd22cS1RUlCIiIpSdne1qO3XqlHJychQXF1ftcahIAABgMn/dtXHixAl9/fXXrq/z8/O1c+dO1a9fX82aNVNKSorS0tIUHR2t6OhopaWlKTg4WIMHD672HCQSAACYzU+ZxLZt29SzZ0/X12f2VwwdOlRLly7VhAkTVFJSojFjxujo0aOKiYnRunXrFBISUu05bIZhGD6P3M92FxT7OwSgRuraZ6K/QwBqnJIdVT+syZdOlvnmV21wrZr3qG0qEgAAmMzK79ogkQAAwGRWfvsnd20AAACvWXKPBGqG0tJSpaenKzU11Wf3PQNWwM8GrIREAqYpKipSWFiYjh07ptDQUH+HA9QY/GzASljaAAAAXiORAAAAXiORAAAAXiORgGnsdrumTJnCZjLgV/jZgJWw2RIAAHiNigQAAPAaiQQAAPAaiQQAAPAaiQQAAPAaiQRM89xzzykqKkp16tRRt27d9OGHH/o7JMCvNm7cqH79+ikyMlI2m02rV6/2d0jAb0YiAVO89tprSklJ0aRJk7Rjxw716NFDycnJOnjwoL9DA/ymuLhYnTp1UmZmpr9DAXyG2z9hipiYGHXt2lXz5s1ztbVt21YDBw5Uenq6HyMDagabzaZVq1Zp4MCB/g4F+E2oSMDnTp06pe3btyspKcmtPSkpSZs3b/ZTVAAAM5BIwOcOHz6s8vJyORwOt3aHwyGn0+mnqAAAZiCRgGlsNpvb14ZhVGoDAFzcSCTgcw0bNlRgYGCl6kNhYWGlKgUA4OJGIgGfq127trp166bs7Gy39uzsbMXFxfkpKgCAGS7xdwCwpnHjxunOO+9U9+7dFRsbqwULFujgwYMaPXq0v0MD/ObEiRP6+uuvXV/n5+dr586dql+/vpo1a+bHyADvcfsnTPPcc88pIyNDBQUF6tChg2bNmqXrrrvO32EBfvPBBx+oZ8+eldqHDh2qpUuXXviAAB8gkQAAAF5jjwQAAPAaiQQAAPAaiQQAAPAaiQQAAPAaiQQAAPAaiQQAAPAaiQQAAPAaiQRgQVOnTlXnzp1dXw8bNkwDBw684HHs379fNptNO3fuvOBzA7gwSCSAC2jYsGGy2Wyy2WyqVauWWrRooYceekjFxcWmzvvMM89U+8mJ/PIH4AnetQFcYDfccIOWLFmisrIyffjhhxoxYoSKi4s1b948t35lZWWqVauWT+YMCwvzyTgA8GtUJIALzG63KyIiQk2bNtXgwYM1ZMgQrV692rUcsXjxYrVo0UJ2u12GYejYsWO65557FB4ertDQUPXq1Uuffvqp25jTpk2Tw+FQSEiI7r77bv30009u53+9tFFRUaHp06erVatWstvtatasmZ5++mlJUlRUlCSpS5custlsSkhIcH1uyZIlatu2rerUqaMrr7xSzz33nNs8H3/8sbp06aI6deqoe/fu2rFjhw+/cwBqIioSgJ8FBQWprKxMkvT1119rxYoVWrlypQIDAyVJffv2Vf369fX2228rLCxMzz//vHr37q0vv/xS9evX14oVKzRlyhTNnTtXPXr00EsvvaRnn31WLVq0OOucqampWrhwoWbNmqVrr71WBQUF+s9//iPpdDJwzTXX6L333lP79u1Vu3ZtSdLChQs1ZcoUZWZmqkuXLtqxY4dGjhypunXraujQoSouLtaf//xn9erVSy+//LLy8/P1wAMPmPzdA+B3BoALZujQocaAAQNcX3/00UdGgwYNjFtuucWYMmWKUatWLaOwsNB1fv369UZoaKjx008/uY3TsmVL4/nnnzcMwzBiY2ON0aNHu52PiYkxOnXqVOW8RUVFht1uNxYuXFhljPn5+YYkY8eOHW7tTZs2NZYtW+bW9uSTTxqxsbGGYRjG888/b9SvX98oLi52nZ83b16VYwGwDpY2gAvsrbfeUr169VSnTh3Fxsbquuuu05w5cyRJV1xxhRo1auTqu337dp04cUINGjRQvXr1XEd+fr727t0rSdq9e7diY2Pd5vj117+0e/dulZaWqnfv3tWO+fvvv9ehQ4d09913u8Xx1FNPucXRqVMnBQcHVysOANbA0gZwgfXs2VPz5s1TrVq1FBkZ6bahsm7dum59Kyoq1LhxY33wwQeVxrn00ku9mj8oKMjjz1RUVEg6vbwRExPjdu7MEoxhGF7FA+DiRiIBXGB169ZVq1atqtW3a9eucjqduuSSS9S8efMq+7Rt21Zbt27VX/7yF1fb1q1bzzpmdHS0goKCtH79eo0YMaLS+TN7IsrLy11tDodDTZo00b59+zRkyJAqx23Xrp1eeukllZSUuJKVc8UBwBpY2gBqsOuvv16xsbEaOHCg3n33Xe3fv1+bN2/Wo48+qm3btkmSHnjgAS1evFiLFy/Wl19+qSlTpigvL++sY9apU0cTJ07UhAkT9OKLL2rv3r3aunWrFi1aJEkKDw9XUFCQ1q5dq++++07Hjh2TdPohV+np6XrmmWf05Zdf6vPPP9eSJUs0c+ZMSdLgwYMVEBCgu+++W7t27dLbb7+tGTNmmPwdAuBvJBJADWaz2fT222/ruuuu0/Dhw9W6dWvddttt2r9/vxwOhyTp1ltv1eTJkzVx4kR169ZNBw4c0L333nvOcR977DGNHz9ekydPVtu2bXXrrbeqsLBQknTJJZfo2Wef1fPPP6/IyEgNGDBAkjRixAi98MILWrp0qTp27Kj4+HgtXbrUdbtovXr19Oabb2rXrl3q0qWLJk2apOnTp5v43QFQE9gMFjYBAICXqEgAAACvkUgAAACvkUgAAACvkUgAAACvkUgAAACvkUgAAACvkUgAAACvkUgAAACvkUgAAACvkUgAAACvkUgAAACvkUgAAACv/T93ykZUxtF0OgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 11. Train a Logistic Regression model and visualize the confusion matrix for binary classification\n",
    "model_binary = LogisticRegression(max_iter=200)\n",
    "model_binary.fit(X_train, y_train)\n",
    "y_pred_binary = model_binary.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_binary)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "484025bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.91\n",
      "Recall: 0.80\n",
      "F1 Score: 0.86\n"
     ]
    }
   ],
   "source": [
    "# 12. Train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "f1 = f1_score(y_test, y_pred_binary)\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c538787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Model Accuracy: 0.86\n"
     ]
    }
   ],
   "source": [
    "# 13. Train a Logistic Regression model on imbalanced data and apply class weights to improve model performance\n",
    "# Create imbalanced dataset\n",
    "X_imbalanced, y_imbalanced = make_classification(n_samples=1000, n_features=20, n_classes=2, weights=[0.9, 0.1], random_state=42)\n",
    "X_train_imb, X_test_imb, y_train_imb, y_test_imb = train_test_split(X_imbalanced, y_imbalanced, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train with class weights\n",
    "model_weighted = LogisticRegression(class_weight='balanced', max_iter=200)\n",
    "model_weighted.fit(X_train_imb, y_train_imb)\n",
    "y_pred_weighted = model_weighted.predict(X_test_imb)\n",
    "\n",
    "# Print accuracy\n",
    "accuracy_weighted = accuracy_score(y_test_imb, y_pred_weighted)\n",
    "print(f\"Weighted Model Accuracy: {accuracy_weighted:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f04a3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation Coefficient (MCC): 0.72\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create a synthetic dataset for demonstration\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# 21. Train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)\n",
    "model_mcc = LogisticRegression(max_iter=200)\n",
    "model_mcc.fit(X_train, y_train)\n",
    "y_pred_mcc = model_mcc.predict(X_test)\n",
    "\n",
    "# Calculate MCC\n",
    "mcc = matthews_corrcoef(y_test, y_pred_mcc)\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ca60b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation Coefficient (MCC): 0.72\n"
     ]
    }
   ],
   "source": [
    "# 22. Train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)\n",
    "model_mcc = LogisticRegression(max_iter=200)\n",
    "model_mcc.fit(X_train, y_train)\n",
    "y_pred_mcc = model_mcc.predict(X_test)\n",
    "\n",
    "# Calculate MCC\n",
    "mcc = matthews_corrcoef(y_test, y_pred_mcc)\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "953dc141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Raw Data: 0.85\n",
      "Accuracy on Standardized Data: 0.85\n"
     ]
    }
   ],
   "source": [
    "# 23. Train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling\n",
    "# Train on raw data\n",
    "model_raw = LogisticRegression(max_iter=200)\n",
    "model_raw.fit(X_train, y_train)\n",
    "y_pred_raw = model_raw.predict(X_test)\n",
    "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
    "print(f\"Accuracy on Raw Data: {accuracy_raw:.2f}\")\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train on standardized data\n",
    "model_scaled = LogisticRegression(max_iter=200)\n",
    "model_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "print(f\"Accuracy on Standardized Data: {accuracy_scaled:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8321dd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 0.01, Cross-Validation Accuracy: 0.87\n",
      "C: 0.1, Cross-Validation Accuracy: 0.87\n",
      "C: 1, Cross-Validation Accuracy: 0.87\n",
      "C: 10, Cross-Validation Accuracy: 0.87\n",
      "C: 100, Cross-Validation Accuracy: 0.87\n",
      "Optimal C: 0.1, Best Cross-Validation Accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "# 24. Train Logistic Regression and find the optimal C (regularization strength) using cross-validation\n",
    "# Define a range of C values\n",
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "best_score = 0\n",
    "best_C = None\n",
    "\n",
    "for C in C_values:\n",
    "    model_cv = LogisticRegression(C=C, max_iter=200)\n",
    "    scores = cross_val_score(model_cv, X_train, y_train, cv=5)\n",
    "    mean_score = np.mean(scores)\n",
    "    print(f\"C: {C}, Cross-Validation Accuracy: {mean_score:.2f}\")\n",
    "    \n",
    "    if mean_score > best_score:\n",
    "        best_score = mean_score\n",
    "        best_C = C\n",
    "\n",
    "print(f\"Optimal C: {best_C}, Best Cross-Validation Accuracy: {best_score:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cca3eef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n",
      "Accuracy of Loaded Model: 0.86\n"
     ]
    }
   ],
   "source": [
    "# 25. Train Logistic Regression, save the trained model using joblib, and load it again to make predictions\n",
    "# Train the model with the optimal C\n",
    "model_final = LogisticRegression(C=best_C, max_iter=200)\n",
    "model_final.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model_final, 'logistic_regression_model.joblib')\n",
    "print(\"Model saved.\")\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('logistic_regression_model.joblib')\n",
    "y_pred_loaded = loaded_model.predict(X_test)\n",
    "\n",
    "# Evaluate the loaded model\n",
    "loaded_accuracy = accuracy_score(y_test, y_pred_loaded)\n",
    "print(f\"Accuracy of Loaded Model: {loaded_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8451108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa043980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

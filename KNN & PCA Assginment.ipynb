{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f169ff6-67b6-42b9-baa1-f0706ed188d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is K‑Nearest Neighbors (KNN) and how does it work?\n",
    "Answer: KNN is a non‑parametric, instance‑based learning algorithm. It classifies or predicts outcomes by finding the K closest\n",
    "data points (neighbors) to a query point using a distance metric, then making decisions based on those neighbors (majority vote for classification, \n",
    "average for regression).\n",
    "\n",
    "===================================================================================================================================\n",
    "Q2. What is the difference between KNN Classification and KNN Regression?\n",
    "Answer:\n",
    "- Classification: Predicts categorical labels using majority voting among neighbors.\n",
    "- Regression: Predicts continuous values using the average (or weighted average) of neighbors.\n",
    "\n",
    "===================================================================================================================================    \n",
    "Q3. What is the role of the distance metric in KNN?\n",
    "Answer: The distance metric (e.g., Euclidean, Manhattan, Minkowski, cosine similarity) determines how “closeness” between points is measured.\n",
    "Choice of metric directly affects neighbor selection and model accuracy.\n",
    "\n",
    "===================================================================================================================================\n",
    "Q4. What is the Curse of Dimensionality in KNN?\n",
    "Answer: As the number of features increases, distances between points become less meaningful (all points appear equally far).\n",
    "    This reduces KNN’s effectiveness and increases computational cost.\n",
    "\n",
    "===================================================================================================================================\n",
    "Q5. How can we choose the best value of K in KNN?\n",
    "Answer:\n",
    "- Use cross‑validation to test different K values.\n",
    "- Small K → sensitive to noise, high variance.\n",
    "- Large K → smoother decision boundary, higher bias.\n",
    "- Often chosen as an odd number to avoid ties.\n",
    "\n",
    "===================================================================================================================================\n",
    "Q6. What are KD Tree and Ball Tree in KNN?\n",
    "Answer:\n",
    "- KD Tree: A binary tree that partitions space along feature axes, efficient for low‑dimensional data.\n",
    "- Ball Tree: A tree that partitions space into hyperspheres (balls), better for high‑dimensional data.\n",
    "\n",
    "===================================================================================================================================\n",
    "Q7. When should you use KD Tree vs. Ball Tree?\n",
    "Answer:\n",
    "- KD Tree: Best for dimensions < 20.\n",
    "- Ball Tree: Preferred for higher dimensions or when data distribution is irregular.\n",
    "\n",
    "===================================================================================================================================\n",
    "Q8. What are the disadvantages of KNN?\n",
    "Answer:\n",
    "- Computationally expensive at prediction time.\n",
    "- Sensitive to irrelevant/noisy features.\n",
    "- Poor performance in high dimensions.\n",
    "- Requires feature scaling.\n",
    "\n",
    "===================================================================================================================================\n",
    "Q9. How does feature scaling affect KNN?\n",
    "Answer: Since KNN relies on distance metrics, features with larger ranges dominate. Scaling (standardization or normalization)\n",
    "    ensures fair contribution of all features.\n",
    "\n",
    "===================================================================================================================================\n",
    "Q10. How does KNN handle missing values in a dataset?\n",
    "Answer: KNN can impute missing values by finding nearest neighbors and replacing missing entries with the mean/median/mode of those neighbors.\n",
    "PCA (Principal Component Analysis)\n",
    "\n",
    "===================================================================================================================================\n",
    "Q11. What is PCA (Principal Component Analysis)?\n",
    "Answer: PCA is a dimensionality reduction technique that transforms correlated features into a smaller set of uncorrelated variables\n",
    "    called principal components, capturing maximum variance.\n",
    "\n",
    "===================================================================================================================================\n",
    "Q12. How does PCA work?\n",
    "Answer:\n",
    "- Standardize data.\n",
    "- Compute covariance matrix.\n",
    "- Find eigenvalues and eigenvectors.\n",
    "- Sort eigenvectors by eigenvalues (variance explained).\n",
    "- Project data onto top components.\n",
    "\n",
    "===================================================================================================================================\n",
    "Q13. What is the geometric intuition behind PCA?\n",
    "Answer: PCA finds new axes (principal components) that maximize variance. Geometrically, it rotates the coordinate system to align with\n",
    "    \\directions of greatest spread in the data.\n",
    "\n",
    "===================================================================================================================================\n",
    "Q14. What is the difference between Feature Selection and Feature Extraction?\n",
    "Answer:\n",
    "- Feature Selection: Chooses a subset of original features.\n",
    "- Feature Extraction: Creates new features (like PCA components) from transformations of original features.\n",
    "\n",
    "===================================================================================================================================\n",
    "Q15. What are Eigenvalues and Eigenvectors in PCA?\n",
    "Answer:\n",
    "- Eigenvectors: Directions of principal components (axes of maximum variance).\n",
    "- Eigenvalues: Magnitude of variance captured along each eigenvector.\n",
    "\n",
    "===================================================================================================================================\n",
    "Q16. How do you decide the number of components to keep in PCA?\n",
    "Answer:\n",
    "- Use explained variance ratio (e.g., keep components explaining 95% variance).\n",
    "- Scree plot (elbow method).\n",
    "- Cross‑validation for downstream tasks.\n",
    "\n",
    "\\===================================================================================================================================\n",
    "Q17. Can PCA be used for classification?\n",
    "Answer: PCA itself is unsupervised, but reduced features can be fed into classifiers (like KNN, SVM) to improve efficiency and reduce overfitting.\n",
    "===================================================================================================================================\n",
    "Q18. What are the limitations of PCA?\n",
    "Answer:\n",
    "- Assumes linear relationships.\n",
    "- Sensitive to scaling.\n",
    "- May discard features important for classification but low in variance.\n",
    "- Hard to interpret transformed components.\n",
    "\n",
    "===================================================================================================================================\n",
    "Q19. How do KNN and PCA complement each other?\n",
    "Answer: PCA reduces dimensionality, mitigating the curse of dimensionality and speeding up KNN. KNN then uses the reduced feature space for\n",
    "                                more effective classification/regression.\n",
    "\n",
    "===================================================================================================================================\n",
    "Q20. What are the key differences between PCA and Linear Discriminant Analysis (LDA)?\n",
    "Answer:\n",
    "- PCA: Unsupervised, maximizes variance, doesn’t use class labels.\n",
    "- LDA: Supervised, maximizes class separability, uses labels to find discriminant axes.\n",
    "- PCA → dimensionality reduction; LDA → classification improvement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bcae43-4e8c-4cff-bf6f-266edc05d7d6",
   "metadata": {},
   "source": [
    "# PRACTICALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e313c1fe-f576-4c98-a8d0-b24ce45fef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q21. Train a KNN Classifier on the Iris dataset and print model accuracy\n",
    "#Answer:\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train KNN Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "#===================================================================================================================================\n",
    "\n",
    "\n",
    "\n",
    "#Q22. Train a KNN Regressor on a synthetic dataset and evaluate using Mean Squared Error (MSE)\n",
    "#Answer:\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Synthetic dataset\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y = np.sin(X).ravel() + np.random.normal(0, 0.1, 100)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train KNN Regressor\n",
    "knn_reg = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = knn_reg.predict(X_test)\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "#===================================================================================================================================\n",
    "\n",
    "\n",
    "#Q23. Train a KNN Classifier using different distance metrics (Euclidean and Manhattan) and compare accuracy\n",
    "#Answer:\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Euclidean distance\n",
    "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "knn_euclidean.fit(X_train, y_train)\n",
    "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
    "print(\"Euclidean Accuracy:\", accuracy_score(y_test, y_pred_euclidean))\n",
    "\n",
    "# Manhattan distance\n",
    "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
    "knn_manhattan.fit(X_train, y_train)\n",
    "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
    "print(\"Manhattan Accuracy:\", accuracy_score(y_test, y_pred_manhattan))\n",
    "\n",
    "\n",
    "#===================================================================================================================================\n",
    "#Q24. Train a KNN Classifier with different values of K and visualize decision boundaries\n",
    "#Answer:\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Use only first two features for visualization\n",
    "X_vis, y_vis = iris.data[:, :2], iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vis, y_vis, test_size=0.3, random_state=42)\n",
    "\n",
    "# Decision boundary plot function\n",
    "def plot_decision_boundary(knn, X, y, title):\n",
    "    h = .02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=ListedColormap(('red','green','blue')))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', cmap=ListedColormap(('red','green','blue')))\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Try different K values\n",
    "for k in [1, 5, 10]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    plot_decision_boundary(knn, X_vis, y_vis, f\"K={k}\")\n",
    "\n",
    "#===================================================================================================================================\n",
    "\n",
    "#Q25. Apply Feature Scaling before training a KNN model and compare results with unscaled data\n",
    "#Answer:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Without scaling\n",
    "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_unscaled.fit(X_train, y_train)\n",
    "print(\"Unscaled Accuracy:\", accuracy_score(y_test, knn_unscaled.predict(X_test)))\n",
    "\n",
    "# With scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "print(\"Scaled Accuracy:\", accuracy_score(y_test, knn_scaled.predict(X_test_scaled)))\n",
    "\n",
    "#===================================================================================================================================\n",
    "#Q26. Train a PCA model on synthetic data and print the explained variance ratio for each component\n",
    "#Answer:\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Synthetic dataset\n",
    "X = np.random.rand(100, 5)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=5)\n",
    "pca.fit(X)\n",
    "\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "\n",
    "#===================================================================================================================================\n",
    "#Q27. Apply PCA before training a KNN Classifier and compare accuracy with and without PCA\n",
    "#Answer:\n",
    "# Without PCA\n",
    "knn_no_pca = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_no_pca.fit(X_train, y_train)\n",
    "print(\"Accuracy without PCA:\", accuracy_score(y_test, knn_no_pca.predict(X_test)))\n",
    "\n",
    "# With PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_pca.fit(X_train_pca, y_train)\n",
    "print(\"Accuracy with PCA:\", accuracy_score(y_test, knn_pca.predict(X_test_pca)))\n",
    "\n",
    "#===================================================================================================================================\n",
    "\n",
    "#Q28. Perform Hyperparameter Tuning on a KNN Classifier using GridSearchCV\n",
    "#Answer:\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Best Accuracy:\", grid.best_score_)\n",
    "\n",
    "\n",
    "#===================================================================================================================================\n",
    "#Q29. Train a KNN Classifier and check the number of misclassified samples\n",
    "#Answer:\n",
    "y_pred = knn.predict(X_test)\n",
    "misclassified = (y_test != y_pred).sum()\n",
    "print(\"Number of misclassified samples:\", misclassified)\n",
    "\n",
    "\n",
    "#===================================================================================================================================\n",
    "#Q30. Train a PCA model and visualize the cumulative explained variance\n",
    "#Answer:\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "pca = PCA().fit(X)\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('PCA - Cumulative Explained Variance')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dcab80-141d-43bd-ba94-8ccf6b652ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

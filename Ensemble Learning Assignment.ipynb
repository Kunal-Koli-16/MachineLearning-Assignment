{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b190ef3-aaf4-40aa-a9c5-a7aee1f76c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q1: Can we use Bagging for regression problems?\n",
    "Answer: Yes, Bagging works for regression. It trains multiple regressors on bootstrapped samples and averages their predictions to reduce variance and improve stability.\n",
    "\n",
    "=====================================================================================================================\n",
    "Q2: What is the difference between multiple model training and single model training?\n",
    "Answer:\n",
    "- Single model training: Relies on one model, which may overfit or underfit.\n",
    "- Multiple model training (ensemble): Combines several models to reduce bias and variance, improving generalization.\n",
    "\n",
    "=====================================================================================================================\n",
    "Q3: Explain the concept of feature randomness in Random Forest.\n",
    "Answer: Random Forest introduces randomness by selecting a random subset of features at each split. This decorrelates trees, increases diversity, and enhances predictive performance.\n",
    "\n",
    "=====================================================================================================================\n",
    "Q4: What is OOB (Out-of-Bag) Score?\n",
    "Answer: The OOB score is an internal validation metric. It uses data not included in the bootstrap sample to test accuracy, giving an unbiased estimate of model performance.\n",
    "\n",
    "=====================================================================================================================\n",
    "Q5: How can you measure the importance of features in a Random Forest model?\n",
    "Answer:\n",
    "- Mean decrease in impurity (Gini importance).\n",
    "- Permutation importance (measuring prediction error increase when feature values are shuffled).\n",
    "\n",
    "=====================================================================================================================\n",
    "Q6: Explain the working principle of a Bagging Classifier.\n",
    "Answer: A Bagging Classifier trains multiple base classifiers on bootstrapped samples and combines their predictions through majority voting, reducing variance and improving accuracy.\n",
    "\n",
    "=====================================================================================================================\n",
    "Q7: How do you evaluate a Bagging Classifierâ€™s performance?\n",
    "Answer: By using metrics like accuracy, precision, recall, F1-score, and OOB score. Cross-validation can also be applied for robust evaluation.\n",
    "\n",
    "=====================================================================================================================\n",
    "Q8: How does a Bagging Regressor work?\n",
    "Answer: It trains multiple regressors on bootstrapped samples and averages their outputs. This reduces variance and improves prediction stability compared to a single regressor.\n",
    "\n",
    "=====================================================================================================================\n",
    "Q9: What is the main advantage of ensemble techniques?\n",
    "Answer: They combine multiple models to achieve higher accuracy, robustness, and generalization compared to individual models.\n",
    "\n",
    "=====================================================================================================================\n",
    "Q10: What is the main challenge of ensemble methods?\n",
    "Answer: They can be computationally expensive, harder to interpret, and may require more memory and training time.\n",
    "\n",
    "=====================================================================================================================\n",
    "Q11: Explain the key idea behind ensemble techniques.\n",
    "Answer: The key idea is to combine multiple weak or strong learners to create a more powerful model that reduces bias, variance, or both.\n",
    "\n",
    "=====================================================================================================================\n",
    "Q12: What is a Random Forest Classifier?\n",
    "Answer: It is an ensemble of decision trees trained on bootstrapped samples with feature randomness, combining predictions via majority voting for classification tasks.\n",
    "\n",
    "=====================================================================================================================\n",
    "Q13: What are the main types of ensemble techniques?\n",
    "Answer:\n",
    "- Bagging (Bootstrap Aggregating).\n",
    "- Boosting (e.g., AdaBoost, Gradient Boosting).\n",
    "- Stacking (meta-learning).\n",
    "\n",
    "=====================================================================================================================\n",
    "Q14: What is ensemble learning in machine learning?\n",
    "Answer: Ensemble learning is the process of combining multiple models to improve predictive performance, robustness, and generalization.\n",
    "\n",
    "=====================================================================================================================\n",
    "Q15: When should we avoid using ensemble methods?\n",
    "Answer: Avoid when:\n",
    "- Data is small and simple (single model suffices).\n",
    "- Interpretability is crucial.\n",
    "- Computational resources are limited.\n",
    "\n",
    "=====================================================================================================================\n",
    "Q16: How does Bagging help in reducing overfitting?\n",
    "Answer: Bagging reduces overfitting by averaging predictions from multiple models trained on different bootstrapped samples, lowering variance.\n",
    "\n",
    "=====================================================================================================================\n",
    "Q17: Why is Random Forest better than a single Decision Tree?\n",
    "Answer: Random Forest reduces overfitting, improves accuracy, and provides more stable predictions by combining multiple decorrelated trees.\n",
    "\n",
    "=====================================================================================================================\n",
    "Q18: What is the role of bootstrap sampling in Bagging?\n",
    "Answer: Bootstrap sampling creates diverse training sets by sampling with replacement, ensuring model diversity and reducing variance.\n",
    "\n",
    "=====================================================================================================================\n",
    "Q19: What are some real-world applications of ensemble techniques?\n",
    "Answer:\n",
    "- Fraud detection in banking.\n",
    "- Spam filtering in emails.\n",
    "- Medical diagnosis.\n",
    "- Stock market prediction.\n",
    "- Recommendation systems.\n",
    "\n",
    "=====================================================================================================================\n",
    "Q20: What is the difference between Bagging and Boosting?\n",
    "Answer:\n",
    "- Bagging: Builds models independently on bootstrapped samples and aggregates results (reduces variance).\n",
    "- Boosting: Builds models sequentially, each correcting errors of the previous one (reduces bias).\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7710c6ea-712e-4294-b717-4c0d56a99d5f",
   "metadata": {},
   "source": [
    "# PRACTICALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3059c508-b467-449f-b7c4-51981a8da815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "MSE: 2987.0073593984966\n",
      "mean radius: 0.0348\n",
      "mean texture: 0.0152\n",
      "mean perimeter: 0.0680\n",
      "mean area: 0.0605\n",
      "mean smoothness: 0.0080\n",
      "mean compactness: 0.0116\n",
      "mean concavity: 0.0669\n",
      "mean concave points: 0.1070\n",
      "mean symmetry: 0.0034\n",
      "mean fractal dimension: 0.0026\n",
      "radius error: 0.0143\n",
      "texture error: 0.0037\n",
      "perimeter error: 0.0101\n",
      "area error: 0.0296\n",
      "smoothness error: 0.0047\n",
      "compactness error: 0.0056\n",
      "concavity error: 0.0058\n",
      "concave points error: 0.0038\n",
      "symmetry error: 0.0035\n",
      "fractal dimension error: 0.0059\n",
      "worst radius: 0.0828\n",
      "worst texture: 0.0175\n",
      "worst perimeter: 0.0808\n",
      "worst area: 0.1394\n",
      "worst smoothness: 0.0122\n",
      "worst compactness: 0.0199\n",
      "worst concavity: 0.0373\n",
      "worst concave points: 0.1322\n",
      "worst symmetry: 0.0082\n",
      "worst fractal dimension: 0.0045\n",
      "Random Forest MSE: 2859.641982706767\n",
      "Decision Tree MSE: 5697.789473684211\n",
      "OOB Score: 0.9533333333333334\n",
      "Accuracy: 1.0\n",
      "Trees: 10, Accuracy: 1.0\n",
      "Trees: 50, Accuracy: 1.0\n",
      "Trees: 100, Accuracy: 1.0\n",
      "Trees: 200, Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kunal\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\kunal\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\kunal\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\kunal\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\kunal\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\kunal\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\kunal\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\kunal\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\kunal\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\kunal\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\kunal\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\kunal\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\kunal\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\kunal\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\kunal\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\kunal\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\kunal\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\kunal\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\kunal\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\kunal\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.9979423868312758\n",
      "age: 0.0575\n",
      "sex: 0.0119\n",
      "bmi: 0.2762\n",
      "bp: 0.0871\n",
      "s1: 0.0473\n",
      "s2: 0.0554\n",
      "s3: 0.0512\n",
      "s4: 0.0271\n",
      "s5: 0.3156\n",
      "s6: 0.0708\n",
      "Bagging Accuracy: 1.0\n",
      "Random Forest Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#Q21: Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy\n",
    "#Answer:\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "bag_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "#====================================================================================================================\n",
    "\n",
    "#Q22: Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)\n",
    "#Answer:\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "bag_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
    "bag_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag_reg.predict(X_test)\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "#=====================================================================================================================\n",
    "#Q23: Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores\n",
    "#Answer:\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X, y)\n",
    "\n",
    "importances = rf_clf.feature_importances_\n",
    "for name, score in zip(load_breast_cancer().feature_names, importances):\n",
    "    print(f\"{name}: {score:.4f}\")\n",
    "\n",
    "#=====================================================================================================================\n",
    "#Q24: Train a Random Forest Regressor and compare its performance with a single Decision Tree\n",
    "#Answer:\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "dt_reg = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "rf_reg.fit(X_train, y_train)\n",
    "dt_reg.fit(X_train, y_train)\n",
    "\n",
    "rf_pred = rf_reg.predict(X_test)\n",
    "dt_pred = dt_reg.predict(X_test)\n",
    "\n",
    "print(\"Random Forest MSE:\", mean_squared_error(y_test, rf_pred))\n",
    "print(\"Decision Tree MSE:\", mean_squared_error(y_test, dt_pred))\n",
    "\n",
    "#=====================================================================================================================\n",
    "#Q25: Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier\n",
    "#Answer:\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
    "rf_clf.fit(X, y)\n",
    "\n",
    "print(\"OOB Score:\", rf_clf.oob_score_)\n",
    "\n",
    "#=====================================================================================================================\n",
    "#Q26: Train a Bagging Classifier using SVM as a base estimator and print accuracy\n",
    "#Answer:\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "bag_svm = BaggingClassifier(estimator=SVC(), n_estimators=20, random_state=42)\n",
    "bag_svm.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag_svm.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "#=====================================================================================================================\n",
    "#Q27: Train a Random Forest Classifier with different numbers of trees and compare accuracy\n",
    "#Answer:\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "for n in [10, 50, 100, 200]:\n",
    "    rf_clf = RandomForestClassifier(n_estimators=n, random_state=42)\n",
    "    rf_clf.fit(X_train, y_train)\n",
    "    y_pred = rf_clf.predict(X_test)\n",
    "    print(f\"Trees: {n}, Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "#=====================================================================================================================\n",
    "#Q28: Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score\n",
    "#Answer:\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "bag_lr = BaggingClassifier(estimator=LogisticRegression(max_iter=1000), n_estimators=20, random_state=42)\n",
    "bag_lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_prob = bag_lr.predict_proba(X_test)[:, 1]\n",
    "print(\"AUC Score:\", roc_auc_score(y_test, y_pred_prob))\n",
    "\n",
    "\n",
    "#=====================================================================================================================\n",
    "#Q29: Train a Random Forest Regressor and analyze feature importance scores\n",
    "#Answer:\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_reg.fit(X, y)\n",
    "\n",
    "importances = rf_reg.feature_importances_\n",
    "for name, score in zip(load_diabetes().feature_names, importances):\n",
    "    print(f\"{name}: {score:.4f}\")\n",
    "\n",
    "\n",
    "#=====================================================================================================================\n",
    "#Q30: Train an ensemble model using both Bagging and Random Forest and compare accuracy\n",
    "#Answer:\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "bag_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "bag_pred = bag_clf.predict(X_test)\n",
    "rf_pred = rf_clf.predict(X_test)\n",
    "\n",
    "print(\"Bagging Accuracy:\", accuracy_score(y_test, bag_pred))\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ee43903-0e64-4b93-867e-96a1d8983dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Best Accuracy: 0.9523809523809524\n",
      "Estimators: 10, MSE: 3237.526541353384\n",
      "Estimators: 50, MSE: 2987.0073593984966\n",
      "Estimators: 100, MSE: 2908.80615037594\n",
      "Misclassified samples: []\n",
      "Decision Tree Accuracy: 1.0\n",
      "Bagging Accuracy: 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Accuracy: 1.0\n",
      "worst area: 0.1394\n",
      "worst concave points: 0.1322\n",
      "mean concave points: 0.1070\n",
      "worst radius: 0.0828\n",
      "worst perimeter: 0.0808\n",
      "Precision: 0.963302752293578\n",
      "Recall: 0.9722222222222222\n",
      "F1-score: 0.967741935483871\n",
      "Max Depth: None, Accuracy: 1.0\n",
      "Max Depth: 5, Accuracy: 1.0\n",
      "Max Depth: 10, Accuracy: 1.0\n",
      "Base Estimator: DecisionTreeRegressor, MSE: 2987.0073593984966\n",
      "Base Estimator: KNeighborsRegressor, MSE: 3140.186131007519\n"
     ]
    }
   ],
   "source": [
    "#Q31: Train a Random Forest Classifier and tune hyperparameters using GridSearchCV\n",
    "#Answer:\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "#=====================================================================================================================\n",
    "\n",
    "#Q32: Train a Bagging Regressor with different numbers of base estimators and compare performance\n",
    "#Answer:\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "for n in [10, 50, 100]:\n",
    "    bag_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=n, random_state=42)\n",
    "    bag_reg.fit(X_train, y_train)\n",
    "    y_pred = bag_reg.predict(X_test)\n",
    "    print(f\"Estimators: {n}, MSE: {mean_squared_error(y_test, y_pred)}\")\n",
    "\n",
    "#=====================================================================================================================\n",
    "\n",
    "#Q33: Train a Random Forest Classifier and analyze misclassified samples\n",
    "#Answer:\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "misclassified = [(i, y_test[i], y_pred[i]) for i in range(len(y_test)) if y_test[i] != y_pred[i]]\n",
    "print(\"Misclassified samples:\", misclassified)\n",
    "\n",
    "#=====================================================================================================================\n",
    "\n",
    "#Q34: Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier\n",
    "#Answer:\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "bag_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
    "\n",
    "dt_clf.fit(X_train, y_train)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, dt_clf.predict(X_test)))\n",
    "print(\"Bagging Accuracy:\", accuracy_score(y_test, bag_clf.predict(X_test)))\n",
    "\n",
    "#=====================================================================================================================\n",
    "\n",
    "#Q35: Train a Random Forest Classifier and visualize the confusion matrix\n",
    "#Answer:\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "#=====================================================================================================================\n",
    "\n",
    "\n",
    "#Q36: Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy\n",
    "#Answer:\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "estimators = [\n",
    "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
    "    ('svm', SVC(probability=True, random_state=42))\n",
    "]\n",
    "\n",
    "stack_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(max_iter=1000))\n",
    "stack_clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Stacking Accuracy:\", accuracy_score(y_test, stack_clf.predict(X_test)))\n",
    "\n",
    "#=====================================================================================================================\n",
    "\n",
    "#Q37: Train a Random Forest Classifier and print the top 5 most important features\n",
    "#Answer:\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X, y)\n",
    "\n",
    "importances = rf_clf.feature_importances_\n",
    "indices = importances.argsort()[-5:][::-1]\n",
    "\n",
    "for i in indices:\n",
    "    print(f\"{load_breast_cancer().feature_names[i]}: {importances[i]:.4f}\")\n",
    "\n",
    "\n",
    "#=====================================================================================================================\n",
    "\n",
    "#Q38: Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score\n",
    "#Answer:\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "bag_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred))\n",
    "\n",
    "#=====================================================================================================================\n",
    "\n",
    "\n",
    "#Q39: Train a Random Forest Classifier and analyze the effect of max_depth on accuracy\n",
    "#Answer:\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "for depth in [None, 5, 10]:\n",
    "    rf_clf = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42)\n",
    "    rf_clf.fit(X_train, y_train)\n",
    "    print(f\"Max Depth: {depth}, Accuracy: {accuracy_score(y_test, rf_clf.predict(X_test))}\")\n",
    "\n",
    "\n",
    "\n",
    "#Q40: Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance\n",
    "#Answer:\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "estimators = [DecisionTreeRegressor(), KNeighborsRegressor()]\n",
    "for est in estimators:\n",
    "    bag_reg = BaggingRegressor(estimator=est, n_estimators=50, random_state=42)\n",
    "    bag_reg.fit(X_train, y_train)\n",
    "    y_pred = bag_reg.predict(X_test)\n",
    "    print(f\"Base Estimator: {est.__class__.__name__}, MSE: {mean_squared_error(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea577361-57d8-4db0-bdff-4027764eaf19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
